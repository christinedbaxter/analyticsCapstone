{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will preprocess the data to make it ready for the model. We will focus on data cleaning, integration, and aggregation. We will discuss methodologies investigated and chosen for handling of missing values, feature engineering, and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load the environment variables\n",
    "load_dotenv(\"../config/.env\")\n",
    "\n",
    "scripts_path = os.getenv(\"SCRIPTS_PATH\")\n",
    "\n",
    "# Add the path to the scripts folder and import the functions\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from raw_data import get_raw_dataframes function\n",
    "from raw_data import get_raw_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "city_data, country_data, weather_data, migraine_data = get_raw_dataframes()\n",
    "\n",
    "# Check the shape of the dataframes\n",
    "city_data.shape, country_data.shape, weather_data.shape, migraine_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Drop Unnecessary Columns/Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 DataFrame: `city_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the `city_data` dataframe\n",
    "city_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping all columns for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 DataFrame: `country_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the `country_data` dataframe\n",
    "country_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keeping* the following columns:\n",
    "- 'country'\n",
    "- 'iso2'\n",
    "- 'iso3'\n",
    "\n",
    "*Removing* the following columns:\n",
    "- 'native_name'\n",
    "- 'population'\n",
    "- 'area'\n",
    "- 'capital'\n",
    "- 'capital_lat'\n",
    "- 'capital_lng'\n",
    "- 'region'\n",
    "- 'continent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not needed for the analysis\n",
    "country_data.drop(columns=['native_name', 'population', 'area', 'capital', 'capital_lat', 'capital_lng', 'region', 'continent'], inplace=True)\n",
    "\n",
    "# Check the shape of the dataframe\n",
    "country_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3 DataFrame: `weather_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the `weather_data` dataframe\n",
    "weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keeping* all columns:\n",
    "- 'station_id'\n",
    "- 'city_name'\n",
    "- 'date'\n",
    "- 'season'\n",
    "- '*_temp_c' (avg, min, max)\n",
    "- 'precipitation_mm'\n",
    "- 'snow_depth_mm'\n",
    "- 'avg_wind_dir_deg'\n",
    "- 'avg_wind_speed_kmh'\n",
    "- 'peak_wind_gust_kmh'\n",
    "- 'avg_sea_level_pres_hpa'\n",
    "- 'sunshine_total_min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.4 DataFrame: `migraine_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the migraine data\n",
    "migraine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Keeping* all columns:\n",
    "- 'measure'\n",
    "- 'location'\n",
    "- 'sex'\n",
    "- 'age'\n",
    "- 'cause'\n",
    "- 'metric'\n",
    "- 'year'\n",
    "- 'val'\n",
    "- 'upper'\n",
    "- 'lower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the measure_name column\n",
    "migraine_data['measure'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the metric_name column\n",
    "migraine_data['metric'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we will only be looking at the prevalence (total # of cases in the population) of headache disorders (i.e., migraine, tension-type headache) and confirmed no other measure name columns are present in our dataset. We will remove all percent and rate values, as we are only interested in the total number of cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to only include the number of headache and migraine cases\n",
    "\n",
    "# Identify indices to drop for both 'metric'\n",
    "metric_indices_to_drop = migraine_data[migraine_data['metric'].isin(['Percent', 'Rate'])].index\n",
    "\n",
    "# Drop rows\n",
    "filtered_migraine_data = migraine_data.drop(metric_indices_to_drop)\n",
    "\n",
    "# Format values in the val, upper, and lower columns to two decimal places\n",
    "pd.set_option('display.float_format', lambda x:'%.2f' % x)\n",
    "\n",
    "filtered_migraine_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Review and Plan for Missing/Zero Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 DataFrame: `city_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nCity Missing Values:\\n\")\n",
    "print(city_data.isnull().sum())\n",
    "\n",
    "# Calculate zero counts for each column\n",
    "print(\"\\nCity Zero Counts:\\n\")\n",
    "zero_counts = (city_data == 0).sum()\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: Merge with `country_data` then merge with `weather_data` to fill in missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 DataFrame: `country_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nCountry Missing Values:\\n\")\n",
    "print(country_data.isnull().sum())\n",
    "\n",
    "# Calculate zero counts for each column\n",
    "print(\"\\nCountry Zero Counts:\\n\")\n",
    "zero_counts = (country_data == 0).sum()\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: Merge with `city_data` then merge with `weather_data` to fill in missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 DataFrame: `weather_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nWeather Missing Values:\\n\")\n",
    "print(weather_data.isnull().sum())\n",
    "\n",
    "# Calculate zero counts for each column\n",
    "print(\"\\nWeather Zero Counts:\\n\")\n",
    "zero_counts = (weather_data == 0).sum()\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: Merge with `city_data` and `country_data` to fill in missing values. Then recheck missing and zero counts after merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 DataFrame: `filtered_migraine_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nFiltered Migraine Missing Values:\\n\")\n",
    "print(filtered_migraine_data.isnull().sum())\n",
    "\n",
    "# Check for zero values\n",
    "print(\"\\nFiltered Migraine Zero Counts:\\n\")\n",
    "zero_counts = (filtered_migraine_data == 0).sum()\n",
    "print(zero_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values in this dataset.\n",
    "\n",
    "Plan: Merge with combined and filtered weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to address the zero values in the `val`, `upper`, and `lower` columns. We will investigate the distribution of these values to determine the best method for handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print rows with zero values\n",
    "zero_rows_any = filtered_migraine_data[(filtered_migraine_data == 0).any(axis=1)]\n",
    "print(zero_rows_any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the counts of the population's age groups\n",
    "filtered_migraine_data['age'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of zero values in the `val`, `upper`, and `lower` columns is 6,100. After further investigation, there are also 6,100 rows where the age of the population is <5 years old. Since this is perfectly reasonable explanation, we will remove these rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that meet the condition\n",
    "filtered_migraine_data.drop(\n",
    "    filtered_migraine_data.query(\"`age` == '<5 years' and `val` == 0\").index, \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_rows_any = filtered_migraine_data[(filtered_migraine_data == 0).any(axis=1)]\n",
    "print(zero_rows_any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Standardize Country and State Names across datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 DataFrame: `city_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.1 Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the function\n",
    "from data_location_matcher import find_matching_and_non_matching\n",
    "\n",
    "# Find matching and non-matching countries\n",
    "city_data_matching_countries, city_data_non_matching_countries = find_matching_and_non_matching(city_data, 'country')\n",
    "\n",
    "# View the matching countries\n",
    "city_data_matching_countries\n",
    "\n",
    "# View the non-matching countries\n",
    "city_data_non_matching_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data_country_replacement_dict = { \n",
    "    'Guinea Bissau': 'Guinea-Bissau',\n",
    "    'Korea, North': 'North Korea',\n",
    "    'Korea, South': 'South Korea',\n",
    "    'Macau S.A.R': 'Macau',\n",
    "    'Svalbard and Jan Mayen Islands': 'Svalbard and Jan Mayen',\n",
    "    'São Tomé and Príncipe': 'Sao Tome and Principe',\n",
    "    'The Bahamas': 'Bahamas',\n",
    "    'The Gambia': 'Gambia',\n",
    "    'United States': 'United States of America'\n",
    "}\n",
    "\n",
    "# Replace the country names in the city dataframe\n",
    "city_data['country'].replace(city_data_country_replacement_dict, inplace=True)\n",
    "\n",
    "city_data['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.1.2 State Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matching and non-matching states\n",
    "city_data_matching_states, city_data_non_matching_states = find_matching_and_non_matching(city_data, 'state')\n",
    "\n",
    "# View the matching states\n",
    "city_data_matching_states\n",
    "\n",
    "# View the non-matching states\n",
    "city_data_non_matching_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 DataFrame: `country_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.1 Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matching and non-matching countries\n",
    "country_data_matching_countries, country_data_non_matching_countries = find_matching_and_non_matching(country_data, 'country')\n",
    "\n",
    "# View the matching countries\n",
    "country_data_matching_countries\n",
    "\n",
    "# View the non-matching countries\n",
    "country_data_non_matching_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data_country_replacement_dict = {\n",
    "    'Democratic Republic of the Congo': 'Congo (Kinshasa)',\n",
    "    'Republic of the Congo': 'Congo (Brazzaville)',\n",
    "    'Korea, North': 'North Korea',\n",
    "    'Korea, South': 'South Korea',\n",
    "    'São Tomé and Príncipe': 'Sao Tome and Principe', \n",
    "    'The Bahamas': 'Bahamas',\n",
    "    'The Gambia': 'Gambia',\n",
    "    'United States': 'United States of America'\n",
    "}\n",
    "\n",
    "# Replace the country names in the country dataframe\n",
    "country_data['country'].replace(country_data_country_replacement_dict, inplace=True)\n",
    "\n",
    "country_data['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.2.2 State Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No state names in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 DataFrame: `migraine_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.1 Country Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_location_matcher import COUNTRIES, US_STATES\n",
    "\n",
    "migraine_data_countries_and_states = filtered_migraine_data['location'].unique()\n",
    "\n",
    "migraine_data_countries_and_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "migraine_data_location_replacement_dict = {\n",
    "    'Taiwan (Province of China)': 'Taiwan',\n",
    "    'Viet Nam': 'Vietnam',\n",
    "    \"Democratic People's Republic of Korea\": 'North Korea',\n",
    "    \"Lao People's Democratic Republic\": 'Laos',\n",
    "    'Democratic Republic of the Congo': 'Congo (Kinshasa)',\n",
    "    'Micronesia (Federated States of)': 'Micronesia',\n",
    "    'North Macedonia': 'Macedonia',\n",
    "    'Brunei Darussalam': 'Brunei',\n",
    "    'Republic of Korea': 'South Korea',\n",
    "    'Bolivia (Plurinational State of)': 'Bolivia',\n",
    "    'Venezuela (Bolivarian Republic of)': 'Venezuela',\n",
    "    'Iran (Islamic Republic of)': 'Iran',\n",
    "    'United Republic of Tanzania': 'Tanzania',    \n",
    "    'Republic of the Congo': 'Congo (Brazzaville)',\n",
    "    'Republic of Moldova': 'Moldova',\n",
    "    'Korea, North': 'North Korea',\n",
    "    'Korea, South': 'South Korea',\n",
    "    'São Tomé and Príncipe': 'Sao Tome and Principe', \n",
    "    'The Bahamas': 'Bahamas',\n",
    "    'The Gambia': 'Gambia',\n",
    "    'United States': 'United States of America'\n",
    "}\n",
    "\n",
    "# Replace the country names in the country dataframe\n",
    "filtered_migraine_data['location'].replace(migraine_data_location_replacement_dict, inplace=True)\n",
    "\n",
    "filtered_migraine_data['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your original list and the list of U.S. states to sets\n",
    "set_migraine_countries_and_states = set(migraine_data_countries_and_states)\n",
    "set_US_states = set(US_STATES)\n",
    "set_countries = set(COUNTRIES)\n",
    "\n",
    "# Create a new list excluding the U.S. states\n",
    "migrained_filtered_countries_list = [item for item in set_migraine_countries_and_states if item not in set_US_states]\n",
    "\n",
    "# View the list\n",
    "migrained_filtered_countries_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.3.3.2 State Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of U.S. states to sets\n",
    "set_US_states = set(US_STATES)\n",
    "\n",
    "# Create a new list including the U.S. states\n",
    "migrained_filtered_states_list = [item for item in set_migraine_countries_and_states if item in set_US_states]\n",
    "\n",
    "# View the list\n",
    "migrained_filtered_states_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly introduce the goal of data integration in the context of this project. Provide a high-level view of the datasets that will be integrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather data provides context regarding sea level pressure, sunshine, temperature, and precipitation for each city. This data is relevant because it provides information about the weather conditions that may be associated with migraine prevalence. The country data will be combined with the city data to provide additional information about each city, such as the country, region, and continent. The combined city and country data will then be combined with the weather data to provide additional information.\n",
    "\n",
    "The daily weather data source file is quite large and is provided in a .parquet format for low memory consumption and data type preservation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.1 Cities DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute            | Description                                        |\n",
    "|----------------------|----------------------------------------------------|\n",
    "| **Data Source Name** | cities.csv                                         |\n",
    "| **Data Source Format** | CSV (comma-separated values)                       |\n",
    "| **Data Source Desc** | Individual cities and weather stations around the world |\n",
    "| **Data Source Size** | 84.1 KB                                             |\n",
    "|                      | 1,245 rows                                          |\n",
    "|                      | 8 columns                                           |\n",
    "| **Data Source Limits** | None                                              |\n",
    "| **Data Source Usability** | 10.00                                          |\n",
    "\n",
    "**Data Source Columns**\n",
    "\n",
    "| Column Name  | Description                               |\n",
    "|--------------|-------------------------------------------|\n",
    "| `station_id` | Unique ID for the weather station.        |\n",
    "| `city_name`  | Name of the city.                         |\n",
    "| `country`    | The country where the city is located.    |\n",
    "| `state`      | The state or province within the country. |\n",
    "| `iso2`       | The two-letter country code.              |\n",
    "| `iso3`       | The three-letter country code.            |\n",
    "| `latitude`   | Latitude coordinate of the city.          |\n",
    "| `longitude`  | Longitude coordinate of the city.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.2 Countries DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute            | Description                                        |\n",
    "|----------------------|----------------------------------------------------|\n",
    "| **Data Source Name** | countries.csv                                         |\n",
    "| **Data Source Format** | CSV (comma-separated values)                       |\n",
    "| **Data Source Desc** | Individual country geographic and demographic characteristics |\n",
    "| **Data Source Size** | 20.6 KB                        |\n",
    "|                      | 214 rows                                    |\n",
    "|                      | 11 columns                                       |\n",
    "| **Data Source Limits** | None                                              |\n",
    "| **Data Source Usability** | 10.00                                           |\n",
    "\n",
    "**Data Source Columns**\n",
    "\n",
    "| Column Name  | Description                                               |\n",
    "|--------------|-----------------------------------------------------------|\n",
    "| `iso3`       | The three-letter code representing the country.           |\n",
    "| `country`    | The English name of the country.                          |\n",
    "| `native_name`| The native name of the country.                           |\n",
    "| `iso2`       | The two-letter code representing the country.             |\n",
    "| `population` | The population of the country.                            |\n",
    "| `area`       | The total land area of the country in square kilometers.  |\n",
    "| `capital`    | The name of the capital city.                             |\n",
    "| `capital_lat`| The latitude coordinate of the capital city.              |\n",
    "| `capital_lng`| The longitude coordinate of the capital city.             |\n",
    "| `region`     | The specific region within the continent where the country is located. |\n",
    "| `continent`  | The continent to which the country belongs.               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1.3 Daily Weather DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute            | Description                                        |\n",
    "|----------------------|----------------------------------------------------|\n",
    "| **Data Source Name** | daily_weather.parquet                              |\n",
    "| **Data Source Format** | .parquet (compressed, maintains original data types, efficient)|\n",
    "| **Data Source Desc** | Daily weather data                            |\n",
    "| **Data Source Size** | 233 MB                                        |\n",
    "|                      | 27,635,763 rows                               |\n",
    "|                      | 14 columns                                    |\n",
    "| **Data Source Limits** | None                                              |\n",
    "| **Data Source Usability** | 10.00                                           |\n",
    "\n",
    "**Data Source Columns**\n",
    "\n",
    "| Column Name            | Description                                       |\n",
    "|------------------------|---------------------------------------------------|\n",
    "| `station_id`           | Unique ID for the weather station.                |\n",
    "| `city_name`            | Name of the city where the station is located.    |\n",
    "| `date`                 | Date of the weather record.                       |\n",
    "| `season`               | Season corresponding to the date (e.g., summer, winter).|\n",
    "| `avg_temp_c`           | Average temperature in Celsius.                   |\n",
    "| `min_temp_c`           | Minimum temperature in Celsius.                   |\n",
    "| `max_temp_c`           | Maximum temperature in Celsius.                   |\n",
    "| `precipitation_mm`     | Precipitation in millimeters.                     |\n",
    "| `snow_depth_mm`        | Snow depth in millimeters.                        |\n",
    "| `avg_wind_dir_deg`     | Average wind direction in degrees.                |\n",
    "| `avg_wind_speed_kmh`   | Average wind speed in kilometers per hour.        |\n",
    "| `peak_wind_gust_kmh`   | Peak wind gust in kilometers per hour.            |\n",
    "| `avg_sea_level_pres_hpa`| Average sea-level pressure in hectopascals.      |\n",
    "| `sunshine_total_min`   | Total sunshine duration in minutes.               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Migraine Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The migraine data provides information about the prevalence of migraine in different countries. This data is relevant because it provides information about the prevalence of migraine by gender, age, year, and location. This data will be combined with the weather data to determine if there is a relationship between weather and migraine prevalence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Attribute            | Description                                        |\n",
    "|----------------------|----------------------------------------------------|\n",
    "| **Data Source Name** | IHME-GBD_2019_DATA-2c1d3941-1.csv                  |\n",
    "|                      | IHME-GBD_2019_DATA-2c1d3941-2.csv                  |\n",
    "|                      | IHME-GBD_2019_DATA-2c1d3941-3.csv                  |\n",
    "| **Data Source Format** | CSV (comma-separated values)                     |\n",
    "| **Data Source Desc** | All GBD causes, risks, impairments, etiologies, and injuries by nature |\n",
    "| **Data Source Size** | 158 MB                                              |\n",
    "|                      | 1,377,000 rows                                      |\n",
    "|                      | 10 columns                                          |\n",
    "| **Data Source Limits** | None                                              |\n",
    "| **Data Source Usability** | 10.00                                          |\n",
    "\n",
    "**Data Source Columns**\n",
    "\n",
    "| Column Name    | Description                                          |\n",
    "|----------------|------------------------------------------------------|\n",
    "| `measure` | The name of measure.                                      |\n",
    "| `location`| The name of each location.                                |\n",
    "| `sex`     | The name of each sex choice.                              |\n",
    "| `age`     | The name of each age group.                               |\n",
    "| `cause`   | The name of each cause.                                   |\n",
    "| `metric`  | The name of each metric/unit.                             |\n",
    "| `year`    | The annual results for all measures.                      |\n",
    "| `val`     | The value of each metric/unit.                            |\n",
    "| `upper`   | The 95% Uncertainty Interval - Upper Bound value.         |\n",
    "| `lower`   | The 95% Uncertainty Interval - Lower Bound value.         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Preliminary Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "In this section, the focus is on preparing the dataset for further analysis and exploration. The steps include merging multiple data sources, filtering the data based on specific criteria, cleaning the data by dropping unnecessary columns and rows, and conducting a preliminary analysis through correlation metrics. Each of these steps is essential for ensuring the data's integrity, usability, and relevance to the study objectives.\n",
    "\n",
    "**5.3.1 Data Merging**\n",
    "\n",
    "The first step involves merging the city and country datasets using a common identifier. This integration provides a comprehensive view that combines geographical and political attributes. Following that, the weather dataset is integrated with the already combined city-country data. The resulting dataset offers a rich context, incorporating both geographical information and meteorological variables.\n",
    "\n",
    "**5.3.2 Data Filtering**\n",
    "\n",
    "The dataset is filtered to only include records pertaining to US cities, thereby narrowing the scope for more targeted analysis. Further filtering is done to include only specific years, enhancing the dataset's relevance to the study period.\n",
    "\n",
    "**5.3.3 Data Cleaning**\n",
    "\n",
    "Columns that do not contribute to the analysis or contain redundant information are dropped to simplify the dataset. Rows with missing or irrelevant data are removed to improve the dataset's quality and consistency. Duplicate rows, if any, are identified and removed to ensure each record in the dataset is unique.\n",
    "\n",
    "**5.3.4 Preliminary Analysis**\n",
    "\n",
    "A correlation analysis is conducted on specific weather attributes like temperature, precipitation, and wind speed to identify any significant relationships among them.\n",
    "\n",
    "---\n",
    "\n",
    "Throughout these steps, the data are continuously inspected to understand their structures, types, and quality. Various data profiling techniques are employed, such as examining data distributions, checking for missing values, and assessing data types, to ensure that the dataset meets the quality and integrity requirements for downstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Data Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1.1 Merge `city_data` and `country_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the countries and cities tables on the `country`, `iso2`, and `iso3` columns to give more context to the weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for joining countries and cities\n",
    "city_country = city_data.merge(country_data, \n",
    "                               how='left', \n",
    "                               left_on=['country', 'iso2', 'iso3'], \n",
    "                               right_on=['country', 'iso2', 'iso3']\n",
    "                               )\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "city_country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1.2 Merge `weather_data` and `city_country`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the weather data with the combined countries and cities tables on the `station_id` and `city_name` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the shape of the weather dataframe\n",
    "print(f\"Weather Data: {weather_data.shape}\")\n",
    "\n",
    "# Review the shape of the city-country dataframe\n",
    "print(f\"City-Country Data: {city_country.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine city/country with daily weather data\n",
    "combined_weather = weather_data.merge(city_country, \n",
    "                                      how='left', \n",
    "                                      left_on=['station_id', 'city_name'], \n",
    "                                      right_on=['station_id', 'city_name']\n",
    ")\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "print(f\"Combined Weather Data: {combined_weather.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the states where country is 'United States of America'\n",
    "combined_weather[combined_weather['country'] == 'United States of America']['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.1.3 Merge `filtered_migraine_data` and `combined_weather_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can merge the migraine data with the weather data, we need to standardize the `location` column in the migraine data. First, we will split the `location` column into `country` and `state` columns. We have already confirmed that the `city_country` dataframe contains both a country and state column so we will use this dataframe to populate the `country` and `state` columns in the migraine data. We will then drop the `location` column from the migraine data. Finally, we will merge the migraine data with the weather data on the `country`, `state`, and `year` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the assign_country function\n",
    "from cleanup_location_migraine import assign_country\n",
    "\n",
    "# Assign country to migraine data\n",
    "filtered_migraine_data['country'] = filtered_migraine_data['location'].apply(assign_country, args=(city_country,))\n",
    "\n",
    "filtered_migraine_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the assign_state function\n",
    "from cleanup_location_migraine import assign_state\n",
    "\n",
    "# Assign state to migraine data\n",
    "filtered_migraine_data['state'] = filtered_migraine_data['location'].apply(assign_state, args=(city_country,))\n",
    "\n",
    "filtered_migraine_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine filtered migraine data with combined weather data\n",
    "weather_migraine_2 = combined_weather.merge(filtered_migraine_data, \n",
    "                                      how='left', \n",
    "                                      left_on=['country'], \n",
    "                                      right_on=['country']\n",
    ")\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "print(f\"Combined Weather and Migraine Data: {weather_migraine_2.shape}\")\n",
    "\n",
    "weather_migraine_2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_migraine_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 Data Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2.1 Filter `combined_weather` by US Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the combined weather data to only include the US\n",
    "usa_weather = combined_weather[combined_weather['iso3'] == 'USA']\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "usa_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View 10 rows of the new dataframe\n",
    "usa_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "usa_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the iso3 column, confirming no other countries are included\n",
    "usa_weather['iso3'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.2.2 Filter `usa_weather` by Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the migraine data is annual, we need to add a 'year' column to the weather data and filter it by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirmed that date column is in datetime format\n",
    "usa_weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather = usa_weather.copy()\n",
    "\n",
    "# Add a column for the year using .loc, specifying to add a new column for all rows\n",
    "usa_weather['year'] = usa_weather['date'].dt.year\n",
    "\n",
    "# Add a column for the month using .loc, specifying to add a new column for all rows\n",
    "usa_weather['month'] = usa_weather['date'].dt.month\n",
    "\n",
    "usa_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather['year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earliest year is 1872, latest year is 2023. We will filter the weather data to only include years 1990-2019 to match the migraine data's date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to only include the years 1990-2019\n",
    "year_filter = (usa_weather['year'] >= 1990) & (usa_weather['year'] <= 2019)\n",
    "usa_weather = usa_weather[year_filter]\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "usa_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather['year'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3 Data Cleaning (2nd Round)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.3.1 Drop Unnecessary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further review of the data, the `country` and `iso2` columns are no longer needed since we have filtered for iso3=USA, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to keep\n",
    "columns_to_keep = [col for col in usa_weather.columns if col not in ['country', 'iso2']]\n",
    "\n",
    "# Use .loc to select only the columns to keep\n",
    "usa_weather = usa_weather.loc[:, columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the shape of the new dataframe\n",
    "usa_weather.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.3.2 Drop Unnecessary Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "usa_weather = usa_weather.dropna(subset=['min_temp_c', 'max_temp_c', 'precipitation_mm'])\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "usa_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "usa_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.3.3 Drop Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original DataFrame\n",
    "usa_weather_row_count = len(usa_weather)\n",
    "\n",
    "# DataFrame after dropping duplicates\n",
    "usa_weather_deduplicated = usa_weather.drop_duplicates()\n",
    "deduplicated_row_count = len(usa_weather_deduplicated)\n",
    "\n",
    "# Calculate the number of rows that would be dropped\n",
    "rows_to_be_dropped = usa_weather_row_count - deduplicated_row_count\n",
    "\n",
    "# Print the difference\n",
    "print(f\"Rows to be dropped: {rows_to_be_dropped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.4 Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3.4.1 Correlation Analysis for Weather Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features = ['avg_temp_c', 'min_temp_c', 'max_temp_c', 'precipitation_mm', 'snow_depth_mm', 'avg_wind_dir_deg',\\\n",
    "                     'avg_wind_speed_kmh', 'peak_wind_gust_kmh', 'avg_sea_level_pres_hpa', 'sunshine_total_min']\n",
    "correlation_matrix_usa_weather = usa_weather[weather_features].corr()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix_usa_weather, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sunshine_total_min` column has a lot of missing values, has a very weak correlation (-0.0066) with `avg_sea_level_pres_hpa`, and is not a focal point of this analysis, so we will drop that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep\n",
    "columns_to_keep = [col for col in usa_weather.columns if col not in ['sunshine_total_min']]\n",
    "\n",
    "# Use .loc to select only the columns to keep\n",
    "usa_weather = usa_weather.loc[:, columns_to_keep]\n",
    "\n",
    "# Check for missing values\n",
    "usa_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Non-pressure-related Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.1.1 Average Temperature Interpolation and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `avg_temp_c` missing values, we will calculate the average of the `min_temp_c` and `max_temp_c` columns and use that value to fill in the missing average temperature values. A new, temporary column will be created called `avg_temp_c_interpolated` to hold these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframe\n",
    "usa_weather = usa_weather.copy()\n",
    "\n",
    "# Create a column for calculating the `avg_temp_c` using the `min_temp_c` and `max_temp_c` columns\n",
    "usa_weather['avg_temp_c_interpolated'] = usa_weather['avg_temp_c'].combine_first\\\n",
    "    ((usa_weather['min_temp_c'] + usa_weather['max_temp_c']) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the mean absolute error (MAE) to determine the accuracy of the interpolated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_absolute_error from sklearn.metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Filter out rows where either of the two columns is NaN\n",
    "filtered_df = usa_weather.dropna(subset=['avg_temp_c', 'avg_temp_c_interpolated'])\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(filtered_df['avg_temp_c'], filtered_df['avg_temp_c_interpolated'])\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(f\"Mean Absolute Error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, the MAE is 0.0, so we will use the interpolated values to fill in the missing values for the `avg_temp_c` column. We will then drop the `avg_temp_c_interpolated` column. Please note after further investigation, it was found that the original `avg_temp_c` values are precisely calculated as the average of `min_temp_c` and `max_temp_c` values, so there is no loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the missing values in the `avg_temp_c` column with the values from the `avg_temp_c_interpolated` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'avg_temp_c' column with the average of the \n",
    "# 'min_temp_c' and 'max_temp_c' columns\n",
    "usa_weather['avg_temp_c'] = usa_weather['avg_temp_c_interpolated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the `avg_temp_c_interpolated` column and check for any remaining missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'avg_temp_c_interpolated' column\n",
    "usa_weather.drop(columns=['avg_temp_c_interpolated'], inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "usa_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(f\"Original Shape: {usa_weather.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.1.2 Aggregate Weather Data by Year and State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The migraine data is aggregated at an annual level and broken down by state, so we need to aggregate the weather data to match. A mean aggregation will be used for all columns except for the `precipitation_mm` and `snow_depth_mm` columns, which will be aggregated using a sum.\n",
    "\n",
    "1. **Group by Year, State, and City Name**: Use pandas' `groupby` method to group data by both the `year`, `state`, and `city` columns.\n",
    "2. **Aggregation Functions**: \n",
    "    - For temperatures (`avg_temp_c`, `min_temp_c`, `max_temp_c`), the mean is calculated for each year and state.\n",
    "    - For wind (`avg_wind_dir_deg`, `avg_wind_speed_kmh`, `peak_wind_gust_kmh`), the mean is calculated for each year and state.\n",
    "    - For `precipitation_mm` and `snow_depth_mm`, the total sum is calculated for each year and state.\n",
    "    - For `avg_sea_level_pres_hpa`, the mean is calculated, assuming it's relevant to have an annual mean sea level pressure for each state.\n",
    "3. **Spatial Data**: For latitude and longitude, the first observed value for each year and state is taken, assuming that these values are consistent within each state and year.\n",
    "\n",
    "By following this methodology, the daily weather data is transformed into an annual summary by state, making it directly comparable with the annual, state-level migraine data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'year' and 'state', then aggregate the numerical columns\n",
    "annual_usa_weather_by_stateCity = usa_weather.groupby(['year', 'state', 'city_name']).agg({\n",
    "    'avg_temp_c': 'mean',\n",
    "    'min_temp_c': 'mean',\n",
    "    'max_temp_c': 'mean',\n",
    "    'precipitation_mm': 'sum',\n",
    "    'snow_depth_mm': 'sum',\n",
    "    'avg_wind_dir_deg': 'mean',\n",
    "    'avg_wind_speed_kmh': 'mean',\n",
    "    'peak_wind_gust_kmh': 'mean',\n",
    "    'avg_sea_level_pres_hpa': 'mean',\n",
    "    'latitude': 'first',  # Assuming all latitudes are the same for a given year and state\n",
    "    'longitude': 'first'  # Assuming all longitudes are the same for a given year and state\n",
    "}).reset_index()\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "annual_usa_weather_by_stateCity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.1.3 Drop Wind Gust and Wind Direction Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.drop(columns=['peak_wind_gust_kmh', 'avg_wind_dir_deg'], inplace=True)\n",
    "\n",
    "annual_usa_weather_by_stateCity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.1.4 Linear Interpolation for Average Wind Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for the `avg_wind_speed_kmh` column utilizing linear interpolation\n",
    "annual_usa_weather_by_stateCity['avg_wind_speed_kmh'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataframe\n",
    "annual_usa_weather_by_stateCity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.2 Pressure-related Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea level pressure can vary greatly depending on the location of the city and the main focus of this analysisis is to see if there is any correlation between sudden changes in sea level pressure and migraines.  As a result, we will not fill in missing values for the `avg_sea_level_pres_hpa` column without further research.  We will work through four different scenarios to determine which seems most accurate for this situation.\n",
    "- Scenario #1: Leave/drop missing values for the `avg_sea_level_pres_hpa` value    \n",
    "- Scenario #2: Utilize linear interpolation to fill in missing values for the `avg_sea_level_pres_hpa` column\n",
    "- Scenario #3: Utilize forward fill to fill in missing values for the `avg_sea_level_pres_hpa` column\n",
    "- Scenario #4: Utilize backward fill to fill in missing values for the `avg_sea_level_pres_hpa` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.1 Leaving/Dropping Missing Values (Scenario #1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for the `avg_sea_level_pres_hpa` column\n",
    "annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.2 Linear Interpolation (Scenario #2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for `avg_sea_level_pres_hpa` column utilizing linear interpolation\n",
    "annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_linear'] = annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'].interpolate(method='linear')\n",
    "\n",
    "# Check for missing values\n",
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.3 Forward Fill (Scenario #3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for `avg_sea_level_pres_hpa` column utilizing forward fill\n",
    "annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_ffill'] = annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'].ffill()\n",
    "\n",
    "# Check for missing values\n",
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.4 Backward Fill (Scenario #4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values for `avg_sea_level_pres_hpa` column utilizing backward fill\n",
    "annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_bfill'] = annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'].bfill()\n",
    "\n",
    "# Check for missing values\n",
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Define common x and y limits\n",
    "x_limits = [1005, 1025]  # Replace with the min and max values across all datasets for the x-axis\n",
    "y_limits = [0, 125]  # Replace with the max frequency across all datasets for the y-axis\n",
    "\n",
    "# Calculate number of bins for each dataset using the Square Root Rule\n",
    "num_bins1 = int(np.sqrt(len(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'].dropna())))\n",
    "num_bins2 = int(np.sqrt(len(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_linear'].dropna())))\n",
    "num_bins3 = int(np.sqrt(len(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_ffill'].dropna())))\n",
    "num_bins4 = int(np.sqrt(len(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_bfill'].dropna())))\n",
    "\n",
    "# Plot each histogram on a different subplot\n",
    "axes[0, 0].hist(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'].dropna(), bins=num_bins1, color='blue')\n",
    "axes[0, 0].set_title('Original')\n",
    "axes[0, 0].set_xlim(x_limits)\n",
    "axes[0, 0].set_ylim(y_limits)\n",
    "\n",
    "axes[0, 1].hist(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_linear'].dropna(), bins=num_bins2, color='green')\n",
    "axes[0, 1].set_title('Linear Interpolated')\n",
    "axes[0, 1].set_xlim(x_limits)\n",
    "axes[0, 1].set_ylim(y_limits)\n",
    "\n",
    "axes[1, 0].hist(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_ffill'].dropna(), bins=num_bins3, color='red')\n",
    "axes[1, 0].set_title('Forward Fill')\n",
    "axes[1, 0].set_xlim(x_limits)\n",
    "axes[1, 0].set_ylim(y_limits)\n",
    "\n",
    "axes[1, 1].hist(annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_bfill'].dropna(), bins=num_bins4, color='purple')\n",
    "axes[1, 1].set_title('Backward Fill')\n",
    "axes[1, 1].set_xlim(x_limits)\n",
    "axes[1, 1].set_ylim(y_limits)\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set(xlabel='Sea Level Pressure (hPa)', ylabel='Frequency')\n",
    "    ax.set_xlim(x_limits)\n",
    "    ax.set_ylim(y_limits)\n",
    "\n",
    "# Display all subplots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.5 Decision on Filling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a thorough review of all four scenarios, we've decided to employ **Scenario #2 (linear interpolation)** for filling the missing values in the `avg_sea_level_pres_hpa` column. The rationale behind this choice is manifold:\n",
    "\n",
    "- **Representation of Data**: Linear interpolation provides a smoother distribution than the other methods. This approach does not heavily skew the tail ends of the distribution, ensuring a more natural representation.\n",
    "  \n",
    "- **Preservation of Original Distribution**: Linear interpolation appears to retain the original data distribution more faithfully when filling in missing values, without introducing any discernible bias towards specific values.\n",
    "\n",
    "- **Percentage of Missing Values**: With only 138 missing values, which represents 9.47% of the total count of 1457, the sheer accuracy of the method is not as paramount as it would be with a more substantial portion of missing values. Nevertheless, it's essential to utilize a method that delivers reliability, and linear interpolation does just that.\n",
    "\n",
    "**Analysis of Alternative Methods:**\n",
    "\n",
    "- The **forward-fill method**, though commendable, might introduce bias as it overlooks subsequent values after a missing point. It stands as our second preference.\n",
    "  \n",
    "- The **backward-fill method** is our third choice. While it does consider subsequent data points, its accuracy seems to trail the forward-fill method.\n",
    "  \n",
    "- Lastly, simply **leaving or dropping missing values** is the least appealing choice, as it disregards the rest of the dataset's information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4.2.6 Linear Interpolation for Average Sea Level Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the dataframe\n",
    "annual_usa_weather_by_stateCity = annual_usa_weather_by_stateCity.copy()\n",
    "\n",
    "# Fill missing values in the `avg_sea_level_pres_hpa` column with the linear values\n",
    "annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa'] = annual_usa_weather_by_stateCity['avg_sea_level_pres_hpa_linear']\n",
    "\n",
    "# Drop the `avg_sea_level_pres_hpa_ffill` and `avg_sea_level_pres_hpa_bfill` columns\n",
    "annual_usa_weather_by_stateCity.drop(columns=['avg_sea_level_pres_hpa_ffill', 'avg_sea_level_pres_hpa_bfill'], inplace=True)\n",
    "\n",
    "# check for missing values\n",
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where either of the two columns is NaN\n",
    "filtered_df = annual_usa_weather_by_stateCity.dropna(subset=['avg_sea_level_pres_hpa', 'avg_sea_level_pres_hpa_linear'])\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(filtered_df['avg_sea_level_pres_hpa'], filtered_df['avg_sea_level_pres_hpa_linear'])\n",
    "\n",
    "# Print the mean absolute error\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.drop(columns=['avg_sea_level_pres_hpa_linear'], inplace=True)\n",
    "\n",
    "annual_usa_weather_by_stateCity.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_usa_weather_by_stateCity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Join Migraine Data with Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the weather data has been aggregated to match the migraine data, we can join the two datasets together. These datasets will be joined on the `year` and `state` columns from both the USA weather data and the migraine data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_migraine_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine USA combined weather with migraine data\n",
    "usa_weather_migraine = annual_usa_weather_by_stateCity.merge(filtered_migraine_data, \n",
    "                                      how='left', \n",
    "                                      left_on=['year', 'state'], \n",
    "                                      right_on=['year', 'state']\n",
    "                                      )\n",
    "\n",
    "# Review the shape of the new dataframe\n",
    "usa_weather_migraine.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussing any new features that were created and why they were created. Also, discuss any features that were dropped and why they were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 Converting Celsius to Fahrenheit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.1 Convert `avg_temp_c` to `avg_temp_f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp_conversion import celsius_to_fahrenheit\n",
    "\n",
    "usa_weather_migraine['avg_temp_f'] = usa_weather_migraine['avg_temp_c'].apply(celsius_to_fahrenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.2 Convert `min_temp_c` to `min_temp_f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather_migraine['min_temp_f'] = usa_weather_migraine['min_temp_c'].apply(celsius_to_fahrenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.3 Convert `max_temp_c` to `max_temp_f`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather_migraine['max_temp_f'] = usa_weather_migraine['max_temp_c'].apply(celsius_to_fahrenheit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.4 Reorder Temperature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder temperature columns\n",
    "temp_col = usa_weather_migraine.pop('avg_temp_f')\n",
    "\n",
    "# Insert columns at new position\n",
    "usa_weather_migraine.insert(3, 'avg_temp_f', temp_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder temperature columns\n",
    "temp_col1 = usa_weather_migraine.pop('min_temp_f')\n",
    "\n",
    "# Insert columns at new position\n",
    "usa_weather_migraine.insert(4, 'min_temp_f', temp_col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder temperature columns\n",
    "temp_col2 = usa_weather_migraine.pop('max_temp_f')\n",
    "\n",
    "# Insert columns at new position\n",
    "usa_weather_migraine.insert(5, 'max_temp_f', temp_col2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather_migraine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1.1.5 Convert `precipitation_mm` to `precipitation_in`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temp_conversion import mM_to_inches\n",
    "\n",
    "usa_weather_migraine['precipitation_in'] = usa_weather_migraine['precipitation_mm'].apply(mM_to_inches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder temperature columns\n",
    "temp_col3 = usa_weather_migraine.pop('precipitation_in')\n",
    "\n",
    "# Insert columns at new position\n",
    "usa_weather_migraine.insert(9, 'precipitation_in', temp_col3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_weather_migraine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 Sea Level Pressure Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `avg_sea_level_pres_hpa`: original column with missing values\n",
    "- `avg_sea_level_pres_hpa_linear`: calculated by using linear interpolation to fill in missing values for the average sea level pressure\n",
    "- `avg_sea_level_pres_hpa_ffill`: calculated by using forward fill to fill in missing values for the average sea level pressure\n",
    "- `avg_sea_level_pres_hpa_bfill`: calculated by using backward fill to fill in missing values for the average sea level pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Dropped Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa_weather_migraine.drop(columns=['avg_temp_c', 'min_temp_c', 'max_temp_c'], inplace=True)\n",
    "\n",
    "# usa_weather_migraine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usa_weather_migraine.drop(columns='precipitation_mm', inplace=True)\n",
    "\n",
    "# usa_weather_migraine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the data preprocessing steps that were taken in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss any next steps that should be taken in the data analysis process/modeling phases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
